<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Fixeds on wil thomason</title>
    <link>https://wbthomason.github.io/fixed/</link>
    <description>Recent content in Fixeds on wil thomason</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 22 Feb 2016 00:22:53 -0500</lastBuildDate>
    
	<atom:link href="https://wbthomason.github.io/fixed/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>publications</title>
      <link>https://wbthomason.github.io/publications/</link>
      <pubDate>Mon, 22 Feb 2016 00:22:53 -0500</pubDate>
      
      <guid>https://wbthomason.github.io/publications/</guid>
      <description>Awards  Outstanding Teaching Assistant Award for my work on Cornell&amp;rsquo;s Foundations of Robotics course (May 2017) NDSEG Fellowship (April 2017) NSF GRFP Fellowship (March 2017) Outstanding Teaching Assistant Award for my work on Cornell&amp;rsquo;s Introductory CS course (May 2016)  Publications  &amp;ldquo;Social Momentum: A Framework for Legible Navigation in Dynamic Multi-Agent Environments&amp;rdquo; Christoforos Mavrogiannis, Wil Thomason, and Ross Knepper. Accepted to HRI 2018 &amp;ldquo;Zero-Shot Learning for Unfamiliar Gesture Recognition&amp;rdquo; Wil Thomason and Ross Knepper.</description>
    </item>
    
    <item>
      <title>projects</title>
      <link>https://wbthomason.github.io/projects/</link>
      <pubDate>Mon, 22 Feb 2016 00:22:49 -0500</pubDate>
      
      <guid>https://wbthomason.github.io/projects/</guid>
      <description>Research  unfamiliar gestures My current research project seeks to create and analyze a framework capable of inferring meaning from unfamiliar human gestures. In interactions between humans, we frequently assume shared contextual knowledge and common grounding. This allows us to make gestures without a guarantee that our conversational partners are familiar with them, but maintain a reasonable level of certainty that we will be understood. Although this is a desirable ability for human-robot interactions, current approaches to understanding gestures, natural language, or the fusion thereof, fall short in this area.</description>
    </item>
    
    <item>
      <title>links</title>
      <link>https://wbthomason.github.io/fixed/links/</link>
      <pubDate>Mon, 22 Feb 2016 00:22:45 -0500</pubDate>
      
      <guid>https://wbthomason.github.io/fixed/links/</guid>
      <description>Links This site contains references to my CV, a list of my publications and presentations, descriptions of a selection of my projects, my (infrequently updated) blog, and my Github profile.</description>
    </item>
    
    <item>
      <title>contact</title>
      <link>https://wbthomason.github.io/contact/</link>
      <pubDate>Mon, 22 Feb 2016 00:22:41 -0500</pubDate>
      
      <guid>https://wbthomason.github.io/contact/</guid>
      <description> Contact  wbthomason ( at ) {my department, abbr.}.{my university}.edu {firstname}(dot){lastname}@{Google&amp;rsquo;s mail service} Twitter: @wilthomason Github: wbthomason Skype: Request via email  </description>
    </item>
    
    <item>
      <title>about</title>
      <link>https://wbthomason.github.io/about/</link>
      <pubDate>Mon, 22 Feb 2016 00:22:20 -0500</pubDate>
      
      <guid>https://wbthomason.github.io/about/</guid>
      <description>I&amp;rsquo;m Wil Thomason, a second-year PhD student in computer science at Cornell. My primary research interests are in robotics, though I&amp;rsquo;m also interested in programming languages, algorithms, machine learning, and interdisciplinary efforts among these areas.
Research I&amp;rsquo;m interested in algorithms for efficient cooperation in heterogeneous multi-agent scenarios - in particular, for ad hoc teams. I want teams of humans and arbitrary robotic systems to be able to work together in a manner which both &amp;ldquo;feels&amp;rdquo; natural and is efficient and effective.</description>
    </item>
    
  </channel>
</rss>